{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [],
   "source": [
    "# Import train and test set\n",
    "train = pd.read_csv('titanic-dataset/train.csv')\n",
    "test = pd.read_csv('titanic-dataset/test.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "outputs": [],
   "source": [
    "# Get target values\n",
    "y = train['Survived']\n",
    "\n",
    "# Drop the Survived column and remove it from further calculations\n",
    "train.drop(columns=['Survived'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "outputs": [
    {
     "data": {
      "text/plain": "(891, 11)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(418, 11)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(891,)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.shape)\n",
    "display(test.shape)\n",
    "display(y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NaN values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "outputs": [
    {
     "data": {
      "text/plain": "PassengerId      0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64"
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "train.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "outputs": [
    {
     "data": {
      "text/plain": "PassengerId                      1\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object"
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use mode() function to get the most frequent value\n",
    "modes = train.mode().iloc[0]\n",
    "modes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "outputs": [],
   "source": [
    "# Use fillna with most frequent values\n",
    "train.fillna(modes, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "train.isna().sum().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get rid of 'Name', 'Ticket' abd 'Cabin' variables\n",
    "**NOTE**: We don't use those values because right now it is out of scope. However, information in those variables are crucial and very imortant! The best score in Kaggle on Titanic dataset is performed only on 'Name' variable! See this notebook for more information: https://www.kaggle.com/code/cdeotte/titanic-using-name-only-0-81818/notebook\n",
    "**NOTE**: It's very common in tabular data to use *categorical embeddings*. This is on *TODO* list right after finishing this notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "outputs": [],
   "source": [
    "train.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "outputs": [
    {
     "data": {
      "text/plain": "     PassengerId  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n0              1       3    male  22.0      1      0   7.2500        S\n1              2       1  female  38.0      1      0  71.2833        C\n2              3       3  female  26.0      0      0   7.9250        S\n3              4       1  female  35.0      1      0  53.1000        S\n4              5       3    male  35.0      0      0   8.0500        S\n..           ...     ...     ...   ...    ...    ...      ...      ...\n886          887       2    male  27.0      0      0  13.0000        S\n887          888       1  female  19.0      0      0  30.0000        S\n888          889       3  female  24.0      1      2  23.4500        S\n889          890       1    male  26.0      0      0  30.0000        C\n890          891       3    male  32.0      0      0   7.7500        Q\n\n[891 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>887</td>\n      <td>2</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>888</td>\n      <td>1</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>889</td>\n      <td>3</td>\n      <td>female</td>\n      <td>24.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>890</td>\n      <td>1</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>891</td>\n      <td>3</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>Q</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore which variables are categorical\n",
    "train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "outputs": [],
   "source": [
    "# Categorical varaibles are Pclass, Sex, Embarked - use one-hot encoding for gender and label for embarked\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "outputs": [
    {
     "data": {
      "text/plain": "     PassengerId  Pclass   Age  SibSp  Parch     Fare  Embarked  Male  Female\n0              1       3  22.0      1      0   7.2500         2   0.0     1.0\n1              2       1  38.0      1      0  71.2833         0   1.0     0.0\n2              3       3  26.0      0      0   7.9250         2   1.0     0.0\n3              4       1  35.0      1      0  53.1000         2   1.0     0.0\n4              5       3  35.0      0      0   8.0500         2   0.0     1.0\n..           ...     ...   ...    ...    ...      ...       ...   ...     ...\n886          887       2  27.0      0      0  13.0000         2   0.0     1.0\n887          888       1  19.0      0      0  30.0000         2   1.0     0.0\n888          889       3  24.0      1      2  23.4500         2   1.0     0.0\n889          890       1  26.0      0      0  30.0000         0   0.0     1.0\n890          891       3  32.0      0      0   7.7500         1   0.0     1.0\n\n[891 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Male</th>\n      <th>Female</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>887</td>\n      <td>2</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>888</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>889</td>\n      <td>3</td>\n      <td>24.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>890</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>891</td>\n      <td>3</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use one_hot_encode for gender\n",
    "gender = one_hot_encoder.fit_transform(train[['Sex']])\n",
    "\n",
    "# Merge train with encoded gender\n",
    "train = train.merge(pd.DataFrame(gender.toarray(), columns=['Male', 'Female']), left_index=True, right_index=True)\n",
    "\n",
    "# Drop unused column\n",
    "train.drop(columns=['Sex'], inplace=True)\n",
    "\n",
    "# Encode 'Embarked' with label encoder\n",
    "train['Embarked'] = label_encoder.fit_transform(train['Embarked'])\n",
    "\n",
    "# Print\n",
    "train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the same preprocessing techniques for test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "outputs": [],
   "source": [
    "test.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "outputs": [
    {
     "data": {
      "text/plain": "     PassengerId  Pclass   Age  SibSp  Parch      Fare  Embarked  Male  Female\n0            892       3  34.5      0      0    7.8292         1   0.0     1.0\n1            893       3  47.0      1      0    7.0000         2   1.0     0.0\n2            894       2  62.0      0      0    9.6875         1   0.0     1.0\n3            895       3  27.0      0      0    8.6625         2   0.0     1.0\n4            896       3  22.0      1      1   12.2875         2   1.0     0.0\n..           ...     ...   ...    ...    ...       ...       ...   ...     ...\n413         1305       3   NaN      0      0    8.0500         2   0.0     1.0\n414         1306       1  39.0      0      0  108.9000         0   1.0     0.0\n415         1307       3  38.5      0      0    7.2500         2   0.0     1.0\n416         1308       3   NaN      0      0    8.0500         2   0.0     1.0\n417         1309       3   NaN      1      1   22.3583         0   0.0     1.0\n\n[418 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Male</th>\n      <th>Female</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>1305</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>1306</td>\n      <td>1</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108.9000</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>415</th>\n      <td>1307</td>\n      <td>3</td>\n      <td>38.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>1308</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>417</th>\n      <td>1309</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>22.3583</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>418 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use one_hot_encode for gender\n",
    "gender = one_hot_encoder.fit_transform(test[['Sex']])\n",
    "\n",
    "# Merge train with encoded gender\n",
    "test = test.merge(pd.DataFrame(gender.toarray(), columns=['Male', 'Female']), left_index=True, right_index=True)\n",
    "\n",
    "# Drop unused column\n",
    "test.drop(columns=['Sex'], inplace=True)\n",
    "\n",
    "# Encode 'Embarked' with label encoder\n",
    "test['Embarked'] = label_encoder.fit_transform(test['Embarked'])\n",
    "\n",
    "# Print\n",
    "test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "outputs": [
    {
     "data": {
      "text/plain": "(891, 9)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(418, 9)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check whether the dimensions are correct\n",
    "display(train.shape)\n",
    "display(test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "outputs": [],
   "source": [
    "n_features = train.shape[1]\n",
    "\n",
    "# Define the class\n",
    "class TitanicNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TitanicNetwork, self).__init__()\n",
    "\n",
    "        # Number of input features is 9\n",
    "        self.layer_in = nn.Linear(n_features, 63)\n",
    "        self.layer_2 = nn.Linear(63, 63)\n",
    "        self.layer_out = nn.Linear(63,1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(63)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(63)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_in(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: that we did not use the Sigmoid activation in our final layer during training. That’s because, we use the nn.BCEWithLogitsLoss() loss function which automatically applies the Sigmoid activation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [
    {
     "data": {
      "text/plain": "TitanicNetwork(\n  (layer_in): Linear(in_features=9, out_features=63, bias=True)\n  (layer_2): Linear(in_features=63, out_features=63, bias=True)\n  (layer_out): Linear(in_features=63, out_features=1, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.1, inplace=False)\n  (batchnorm1): BatchNorm1d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (batchnorm2): BatchNorm1d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)"
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = TitanicNetwork()\n",
    "\n",
    "# Specify the device type responsible to load model into memory\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TitanicNetwork(\n",
      "  (layer_in): Linear(in_features=9, out_features=63, bias=True)\n",
      "  (layer_2): Linear(in_features=63, out_features=63, bias=True)\n",
      "  (layer_out): Linear(in_features=63, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer_in.weight', Parameter containing:\n",
      "tensor([[ 0.1907, -0.1229, -0.2440,  0.1249,  0.2736, -0.2586, -0.1627,  0.0092,\n",
      "          0.3040],\n",
      "        [-0.1466,  0.2820,  0.1667,  0.0917,  0.2914,  0.0559,  0.2692, -0.1819,\n",
      "          0.3111],\n",
      "        [-0.0965, -0.1128,  0.2828, -0.1476, -0.2080,  0.2445,  0.2947,  0.1941,\n",
      "         -0.1125],\n",
      "        [-0.2062,  0.1308,  0.2769,  0.3117,  0.1527, -0.0103,  0.2860,  0.3032,\n",
      "          0.0192],\n",
      "        [ 0.3093,  0.0769, -0.1166,  0.1175,  0.2519,  0.2626, -0.0088,  0.2218,\n",
      "          0.1520],\n",
      "        [ 0.1173,  0.0526,  0.1217,  0.0748, -0.1409, -0.2385, -0.0616,  0.0581,\n",
      "          0.0655],\n",
      "        [ 0.0130, -0.0440,  0.1914, -0.2250,  0.1966, -0.0395, -0.2108,  0.1710,\n",
      "         -0.1307],\n",
      "        [ 0.1630,  0.0220,  0.0566, -0.1350,  0.0823, -0.1331,  0.0939,  0.2427,\n",
      "          0.2522],\n",
      "        [ 0.1909,  0.1631,  0.0541,  0.0383,  0.0873, -0.2937,  0.1262, -0.1916,\n",
      "          0.1727],\n",
      "        [-0.2571, -0.2144,  0.0708,  0.2895,  0.1530,  0.3241, -0.3243,  0.0118,\n",
      "         -0.2121],\n",
      "        [-0.2741,  0.2916,  0.3248,  0.0077, -0.2013,  0.2544, -0.0067, -0.0754,\n",
      "          0.1909],\n",
      "        [ 0.2601, -0.1412,  0.0830, -0.0402,  0.2711,  0.3107, -0.0506,  0.0216,\n",
      "          0.2595],\n",
      "        [ 0.3313, -0.2910, -0.2737, -0.0072, -0.2757, -0.1428, -0.0294,  0.0156,\n",
      "         -0.2065],\n",
      "        [ 0.2153,  0.1781, -0.0147, -0.1010, -0.0473, -0.2587, -0.1898,  0.2307,\n",
      "         -0.2389],\n",
      "        [-0.1763, -0.2867, -0.1702, -0.1012,  0.0032,  0.2607,  0.2698, -0.1876,\n",
      "          0.0809],\n",
      "        [-0.1299, -0.2477,  0.2942, -0.2615, -0.1931, -0.1531,  0.0722,  0.3090,\n",
      "         -0.2234],\n",
      "        [-0.2251, -0.1958, -0.2648,  0.1713,  0.0495,  0.0705, -0.0397,  0.1982,\n",
      "         -0.1592],\n",
      "        [ 0.1071,  0.1018,  0.3213,  0.2419,  0.0475, -0.2499, -0.2789, -0.0733,\n",
      "         -0.2385],\n",
      "        [ 0.2040, -0.1340, -0.2639, -0.2351,  0.1453,  0.3129, -0.1708,  0.0197,\n",
      "          0.0276],\n",
      "        [ 0.0397,  0.2859, -0.1285,  0.2058, -0.0382,  0.0311,  0.1353,  0.2163,\n",
      "          0.2344],\n",
      "        [-0.3125, -0.3292, -0.0054,  0.0942,  0.3321, -0.2565,  0.2364, -0.0118,\n",
      "          0.2145],\n",
      "        [ 0.1293, -0.2897,  0.1495, -0.2226,  0.2358, -0.1040, -0.3307, -0.2450,\n",
      "          0.2332],\n",
      "        [ 0.3249, -0.1889, -0.0722,  0.1065, -0.0238, -0.3059, -0.2912,  0.2970,\n",
      "         -0.2179],\n",
      "        [-0.2801,  0.1930,  0.2254, -0.2526, -0.1094,  0.0600,  0.0123, -0.0200,\n",
      "          0.3322],\n",
      "        [-0.3297, -0.1904, -0.1683,  0.2552, -0.2511, -0.1294,  0.0660, -0.1105,\n",
      "          0.1167],\n",
      "        [-0.2249,  0.1828, -0.2674,  0.1642, -0.1943,  0.0479, -0.2901, -0.1303,\n",
      "          0.2997],\n",
      "        [-0.2047,  0.1379, -0.3094, -0.1191, -0.0227, -0.0678, -0.0193, -0.0201,\n",
      "         -0.1591],\n",
      "        [ 0.1364, -0.0421,  0.2137,  0.0580, -0.0272,  0.0867,  0.2630,  0.3316,\n",
      "          0.0611],\n",
      "        [-0.2386, -0.0725,  0.2075,  0.0846,  0.1264, -0.1638, -0.0069,  0.2927,\n",
      "          0.2345],\n",
      "        [ 0.0068,  0.2029, -0.1316, -0.2162,  0.1471,  0.0095,  0.0264, -0.2916,\n",
      "         -0.1567],\n",
      "        [-0.2908,  0.1290,  0.0380, -0.1574,  0.1110,  0.0548,  0.1437,  0.2824,\n",
      "         -0.2276],\n",
      "        [-0.1821,  0.0139, -0.1799, -0.2803, -0.2586,  0.1268, -0.2559, -0.1884,\n",
      "         -0.3203],\n",
      "        [-0.2402,  0.0409,  0.2827,  0.2382,  0.0939, -0.0583, -0.0522, -0.3077,\n",
      "         -0.1433],\n",
      "        [ 0.2517, -0.1548, -0.2938,  0.2707, -0.3089,  0.1492,  0.0305, -0.1566,\n",
      "          0.1436],\n",
      "        [-0.0385,  0.3013, -0.2037, -0.1468, -0.1862,  0.2625, -0.0976,  0.1026,\n",
      "          0.3126],\n",
      "        [ 0.1258, -0.1851,  0.1924, -0.2916,  0.0975, -0.0736,  0.0447, -0.2240,\n",
      "          0.1261],\n",
      "        [ 0.2634, -0.2556,  0.0254, -0.0974,  0.1776,  0.0095, -0.0595, -0.0528,\n",
      "          0.2043],\n",
      "        [-0.0706,  0.1677,  0.2980,  0.2246, -0.2053, -0.0357, -0.2087, -0.0112,\n",
      "         -0.2815],\n",
      "        [-0.2180,  0.3008, -0.2190, -0.1971, -0.2902,  0.0259, -0.3015, -0.0203,\n",
      "         -0.0945],\n",
      "        [-0.2394,  0.1991,  0.2070,  0.0718, -0.2275,  0.1266,  0.0744,  0.0635,\n",
      "         -0.3317],\n",
      "        [ 0.2687,  0.1259, -0.3271, -0.1433, -0.3051, -0.0308, -0.1203, -0.0084,\n",
      "         -0.0149],\n",
      "        [ 0.3033, -0.2770, -0.2078,  0.3110,  0.2699,  0.0818, -0.0847, -0.1107,\n",
      "         -0.0819],\n",
      "        [ 0.2263,  0.0679, -0.1888, -0.0214, -0.3127,  0.1716, -0.2017, -0.3079,\n",
      "          0.2281],\n",
      "        [-0.1878, -0.2107,  0.0309, -0.1295, -0.1239,  0.0565,  0.2787, -0.1597,\n",
      "          0.3169],\n",
      "        [ 0.1746, -0.0927, -0.1576,  0.2474, -0.1362,  0.2460,  0.2270,  0.0277,\n",
      "          0.0989],\n",
      "        [-0.2674,  0.1181,  0.2606, -0.0217, -0.2813, -0.0296, -0.3046,  0.0074,\n",
      "          0.0890],\n",
      "        [ 0.2780, -0.2701,  0.0598,  0.1034, -0.2540,  0.2113,  0.1510,  0.2591,\n",
      "          0.1018],\n",
      "        [-0.1185,  0.3169,  0.1730,  0.1344,  0.1082,  0.0181,  0.1094,  0.3259,\n",
      "          0.1903],\n",
      "        [ 0.1826,  0.2797,  0.0619, -0.0402, -0.1014,  0.3075,  0.1614, -0.0421,\n",
      "         -0.1127],\n",
      "        [-0.0324, -0.1996,  0.1676, -0.1661, -0.0947, -0.0909,  0.0059,  0.1378,\n",
      "         -0.2750],\n",
      "        [-0.2200,  0.2002, -0.3238,  0.1686, -0.2531,  0.1673, -0.0564,  0.1441,\n",
      "          0.0547],\n",
      "        [-0.3049, -0.2088,  0.3186, -0.0385, -0.0279,  0.0118, -0.1595, -0.2146,\n",
      "          0.1404],\n",
      "        [ 0.2849, -0.2529, -0.2095, -0.3268, -0.1885,  0.0781,  0.2591, -0.2791,\n",
      "          0.0603],\n",
      "        [ 0.0823, -0.1096, -0.3238,  0.0390,  0.0858,  0.1261,  0.0275, -0.2640,\n",
      "          0.3111],\n",
      "        [-0.0935, -0.3023, -0.2459, -0.2773,  0.2245,  0.0723, -0.3005, -0.0166,\n",
      "          0.0310],\n",
      "        [ 0.0394,  0.0662, -0.0055, -0.0442, -0.1631, -0.0316,  0.1896,  0.0045,\n",
      "         -0.3252],\n",
      "        [ 0.3316,  0.2493,  0.3235, -0.0810,  0.0476, -0.2944,  0.0510, -0.3323,\n",
      "          0.0437],\n",
      "        [ 0.3219, -0.3010, -0.2342, -0.2083,  0.0288,  0.3105,  0.1014, -0.0824,\n",
      "          0.0421],\n",
      "        [-0.1987, -0.2526,  0.2744, -0.0250,  0.1665, -0.3039,  0.2626, -0.0759,\n",
      "         -0.1282],\n",
      "        [-0.2493, -0.1362,  0.2067,  0.1566,  0.0276,  0.1067,  0.2542, -0.0887,\n",
      "         -0.0215],\n",
      "        [-0.1963,  0.1295,  0.0064,  0.2238, -0.3157, -0.0449, -0.2273, -0.2963,\n",
      "          0.0090],\n",
      "        [-0.0073,  0.0826,  0.0371, -0.2024, -0.0415,  0.0685, -0.1910, -0.0625,\n",
      "          0.1838],\n",
      "        [-0.2571, -0.1844, -0.3190,  0.0768, -0.3000,  0.0510, -0.0399, -0.2794,\n",
      "         -0.3078]], requires_grad=True)), ('layer_in.bias', Parameter containing:\n",
      "tensor([-0.2591,  0.2248, -0.1217,  0.1838,  0.2028, -0.2820, -0.1611, -0.0534,\n",
      "        -0.1501,  0.3213,  0.2004,  0.1601, -0.1088, -0.0764,  0.1635, -0.1102,\n",
      "         0.1923, -0.1553,  0.2075, -0.1489, -0.2315, -0.2470,  0.2468, -0.1508,\n",
      "        -0.0551,  0.2789,  0.2659, -0.1830, -0.1967,  0.2731, -0.3021, -0.2584,\n",
      "         0.3171, -0.2171, -0.0559, -0.3063,  0.0396, -0.3026,  0.0841,  0.1310,\n",
      "         0.1442, -0.2741,  0.0509, -0.3090,  0.0064, -0.1583,  0.1781,  0.2306,\n",
      "         0.0686, -0.3314,  0.2551, -0.0938, -0.1726, -0.3152, -0.0339,  0.2477,\n",
      "         0.0017,  0.0020, -0.2382,  0.1991,  0.0921, -0.2073, -0.2471],\n",
      "       requires_grad=True)), ('layer_2.weight', Parameter containing:\n",
      "tensor([[ 0.0149, -0.0439,  0.0489,  ..., -0.0038, -0.0290,  0.0228],\n",
      "        [-0.0951, -0.0987,  0.0841,  ..., -0.1081,  0.0190, -0.1123],\n",
      "        [ 0.0682, -0.0304,  0.0667,  ...,  0.1090, -0.0456, -0.0719],\n",
      "        ...,\n",
      "        [-0.1251, -0.0990,  0.0279,  ..., -0.1057,  0.0393, -0.0571],\n",
      "        [-0.0946,  0.0788, -0.1154,  ..., -0.0918,  0.1024, -0.1212],\n",
      "        [-0.1140,  0.0490, -0.0877,  ...,  0.1082, -0.1075,  0.1136]],\n",
      "       requires_grad=True)), ('layer_2.bias', Parameter containing:\n",
      "tensor([-0.0771,  0.0565,  0.0135, -0.0141, -0.0508, -0.1168, -0.0948, -0.0966,\n",
      "         0.1044,  0.0820,  0.0395,  0.0280,  0.0806,  0.0885,  0.0679,  0.0358,\n",
      "         0.0467,  0.1202,  0.0012, -0.0424,  0.0528, -0.0035,  0.0605, -0.0571,\n",
      "        -0.0426,  0.0925, -0.0134,  0.0881,  0.0611, -0.0492, -0.0973, -0.1225,\n",
      "         0.0851,  0.0756, -0.0522, -0.0558, -0.1064,  0.0824,  0.0123,  0.0330,\n",
      "        -0.0974,  0.0780, -0.0753, -0.1066,  0.0241,  0.1196,  0.1137,  0.1250,\n",
      "        -0.0962,  0.0554, -0.0308,  0.0734, -0.0452,  0.0989, -0.1052, -0.0852,\n",
      "        -0.0705, -0.0227,  0.1110, -0.1030, -0.0142,  0.1169, -0.0187],\n",
      "       requires_grad=True)), ('layer_out.weight', Parameter containing:\n",
      "tensor([[-0.0248,  0.0251, -0.0258,  0.1155, -0.0825,  0.1243,  0.0549, -0.1237,\n",
      "         -0.0179, -0.0646,  0.0743,  0.0498, -0.1232, -0.0246, -0.0581,  0.0650,\n",
      "          0.0433, -0.0306,  0.1243,  0.1233,  0.0305, -0.1112,  0.1194, -0.0520,\n",
      "          0.1132, -0.0596,  0.0469,  0.0784,  0.0215, -0.0980,  0.0906, -0.0647,\n",
      "         -0.0484, -0.0697, -0.1022,  0.0912,  0.0664,  0.0787,  0.0051,  0.0253,\n",
      "          0.0677, -0.0187,  0.0392,  0.1095,  0.1034, -0.0391,  0.0335,  0.0485,\n",
      "         -0.0251,  0.0968, -0.0889,  0.1153, -0.0861,  0.1057,  0.0303,  0.0261,\n",
      "          0.0719, -0.0590, -0.0009, -0.0218, -0.0084, -0.0124, -0.1229]],\n",
      "       requires_grad=True)), ('layer_out.bias', Parameter containing:\n",
      "tensor([-0.0525], requires_grad=True)), ('batchnorm1.weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)), ('batchnorm1.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)), ('batchnorm2.weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)), ('batchnorm2.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [],
   "source": [
    "# Init loss function (Binary Cross Entropy Loss) (we assume that target is equally distributed)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train-test-split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.33, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DataLoaders and Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before training, we should implement custom Dataset class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [],
   "source": [
    "class TitanicTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Return the single observation, including both independent and dependent variable \"\"\"\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        # if isinstance(index, torch.Tensor):\n",
    "        #     index = index.tolist()\n",
    "\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        # return [self.X_data.iloc[index].values, self.y_data[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the number of rows from tabular data \"\"\"\n",
    "        return len(self.X_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Return the single observation, including only independent variable \"\"\"\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = index.tolist()\n",
    "\n",
    "        return self.X_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the number of rows from tabular data \"\"\"\n",
    "        return len(self.X_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "outputs": [],
   "source": [
    "train_data = TitanicTrainDataset(torch.Tensor(X_train.values), torch.Tensor(y_train.values))\n",
    "test_data = TitanicTestDataset(torch.Tensor(X_test.values))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "outputs": [
    {
     "data": {
      "text/plain": "596"
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test len method\n",
    "train_data.__len__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "outputs": [
    {
     "data": {
      "text/plain": "0    374\n1    222\nName: Survived, dtype: int64"
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([719.0000,   3.0000,  24.0000,   0.0000,   0.0000,  15.5000,   1.0000,\n           0.0000,   1.0000]),\n tensor(0.))"
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test getitem method\n",
    "train_data.__getitem__(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "outputs": [],
   "source": [
    "# Initialize dataloaders\n",
    "BATCH_SIZE_TRAIN = 64\n",
    "BATCH_SIZE_TEST = 1\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "outputs": [],
   "source": [
    "# Define binary accuracy function\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.67724 | Acc: 61.600\n",
      "Epoch 002: | Loss: 0.65114 | Acc: 62.300\n",
      "Epoch 003: | Loss: 0.62826 | Acc: 67.400\n",
      "Epoch 004: | Loss: 0.61366 | Acc: 65.300\n",
      "Epoch 005: | Loss: 0.60292 | Acc: 69.800\n",
      "Epoch 006: | Loss: 0.60986 | Acc: 68.000\n",
      "Epoch 007: | Loss: 0.59767 | Acc: 69.600\n",
      "Epoch 008: | Loss: 0.60296 | Acc: 69.600\n",
      "Epoch 009: | Loss: 0.57120 | Acc: 72.200\n",
      "Epoch 010: | Loss: 0.61025 | Acc: 67.600\n",
      "Epoch 011: | Loss: 0.59133 | Acc: 70.300\n",
      "Epoch 012: | Loss: 0.58224 | Acc: 70.500\n",
      "Epoch 013: | Loss: 0.59920 | Acc: 68.500\n",
      "Epoch 014: | Loss: 0.58636 | Acc: 69.800\n",
      "Epoch 015: | Loss: 0.58668 | Acc: 67.900\n",
      "Epoch 016: | Loss: 0.57719 | Acc: 69.500\n",
      "Epoch 017: | Loss: 0.57444 | Acc: 69.500\n",
      "Epoch 018: | Loss: 0.56106 | Acc: 72.100\n",
      "Epoch 019: | Loss: 0.54820 | Acc: 72.800\n",
      "Epoch 020: | Loss: 0.54852 | Acc: 71.200\n",
      "Epoch 021: | Loss: 0.55163 | Acc: 72.800\n",
      "Epoch 022: | Loss: 0.51948 | Acc: 73.600\n",
      "Epoch 023: | Loss: 0.51147 | Acc: 76.600\n",
      "Epoch 024: | Loss: 0.50416 | Acc: 75.200\n",
      "Epoch 025: | Loss: 0.49712 | Acc: 77.500\n",
      "Epoch 026: | Loss: 0.49139 | Acc: 77.200\n",
      "Epoch 027: | Loss: 0.46255 | Acc: 79.300\n",
      "Epoch 028: | Loss: 0.46541 | Acc: 78.100\n",
      "Epoch 029: | Loss: 0.46014 | Acc: 78.400\n",
      "Epoch 030: | Loss: 0.47976 | Acc: 77.800\n",
      "Epoch 031: | Loss: 0.43643 | Acc: 82.000\n",
      "Epoch 032: | Loss: 0.46333 | Acc: 78.700\n",
      "Epoch 033: | Loss: 0.41351 | Acc: 81.800\n",
      "Epoch 034: | Loss: 0.44174 | Acc: 80.400\n",
      "Epoch 035: | Loss: 0.44462 | Acc: 80.300\n",
      "Epoch 036: | Loss: 0.43463 | Acc: 81.900\n",
      "Epoch 037: | Loss: 0.44751 | Acc: 80.400\n",
      "Epoch 038: | Loss: 0.42347 | Acc: 82.000\n",
      "Epoch 039: | Loss: 0.44126 | Acc: 79.100\n",
      "Epoch 040: | Loss: 0.40988 | Acc: 82.500\n",
      "Epoch 041: | Loss: 0.41753 | Acc: 81.600\n",
      "Epoch 042: | Loss: 0.42879 | Acc: 80.500\n",
      "Epoch 043: | Loss: 0.42229 | Acc: 80.400\n",
      "Epoch 044: | Loss: 0.41589 | Acc: 82.000\n",
      "Epoch 045: | Loss: 0.40718 | Acc: 82.100\n",
      "Epoch 046: | Loss: 0.43119 | Acc: 81.100\n",
      "Epoch 047: | Loss: 0.42009 | Acc: 80.900\n",
      "Epoch 048: | Loss: 0.42550 | Acc: 81.200\n",
      "Epoch 049: | Loss: 0.39772 | Acc: 83.900\n",
      "Epoch 050: | Loss: 0.41135 | Acc: 82.600\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# Prepare model for training (default state)\n",
    "model.train()\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "\n",
    "    # Init loss and acc per epoch to zero\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # For loop to get our data in batches from dataloader\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "\n",
    "        # Load batches into memory (device)\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Set .grad attribute of all tensors to zero (otherwise we would accumulate it with .backwards())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (use input data to make prediction)\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # Calculate loss based on prediction and true value\n",
    "        loss = loss_function(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "        # We backpropagate this error through the network\n",
    "        # The gradient is calculated for tensors which requires_grad=True\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust each parameter (weight and bias) by its gradient stored in .grad\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_dataloader):.5f} | Acc: {epoch_acc/len(train_dataloader):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "\n",
    "# Prepare model for testing\n",
    "model.eval()\n",
    "\n",
    "# We don't want to perform backpropagation\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Go through batches\n",
    "    for X_batch in test_dataloader:\n",
    "\n",
    "        # Load batch into memory\n",
    "        X_batch = X_batch.to(device)\n",
    "\n",
    "        # Get the result\n",
    "        y_test_pred = model(X_batch)\n",
    "\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "\n",
    "        # Round probabilities to 0 or 1\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "\n",
    "        # Convert tensor to numpy object\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "# Flatten the list to use confusion matrix and classification report\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[158,  17],\n       [ 41,  79]])"
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Plot confusion matrix\n",
    "confusion_matrix(y_test, y_pred_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84       175\n",
      "           1       0.82      0.66      0.73       120\n",
      "\n",
      "    accuracy                           0.80       295\n",
      "   macro avg       0.81      0.78      0.79       295\n",
      "weighted avg       0.81      0.80      0.80       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
